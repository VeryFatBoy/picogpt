{"cells":[{"attachments":{},"cell_type":"markdown","id":"b5989e16-81a8-4619-9684-9a330e76fe39","metadata":{"language":"python"},"source":"<img src = \"https://github.com/VeryFatBoy/notebooks/blob/main/common/images/img_github_singlestore-jupyter_featured_2.png?raw=true\">"},{"attachments":{},"cell_type":"markdown","id":"c713f283-94bc-4be7-8f70-656639c6a870","metadata":{"language":"python"},"source":"<div id=\"singlestore-header\" style=\"display: flex; background-color: rgba(235, 249, 245, 0.25); padding: 5px;\">\n    <div id=\"icon-image\" style=\"width: 90px; height: 90px;\">\n        <img width=\"100%\" height=\"100%\" src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/header-icons/browser.png\" />\n    </div>\n    <div id=\"text\" style=\"padding: 5px; margin-left: 10px;\">\n        <div id=\"badge\" style=\"display: inline-block; background-color: rgba(0, 0, 0, 0.15); border-radius: 4px; padding: 4px 8px; align-items: center; margin-top: 6px; margin-bottom: -2px; font-size: 80%\">SingleStore Notebooks</div>\n        <h1 style=\"font-weight: 500; margin: 8px 0 0 4px;\">picoGPT</h1>\n    </div>\n</div>"},{"cell_type":"code","execution_count":4,"id":"b2437fb8-cd0c-42a4-8d26-022116ed8c49","metadata":{"execution":{"iopub.execute_input":"2024-09-23T23:08:17.858379Z","iopub.status.busy":"2024-09-23T23:08:17.858002Z","iopub.status.idle":"2024-09-23T23:08:20.780193Z","shell.execute_reply":"2024-09-23T23:08:20.778066Z","shell.execute_reply.started":"2024-09-23T23:08:17.858346Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Files removed: 196\n"}],"source":"!pip cache purge"},{"cell_type":"code","execution_count":5,"id":"cf1c304b-8f69-47b2-ac52-b3021f52fedf","metadata":{"execution":{"iopub.execute_input":"2024-09-23T23:08:24.701571Z","iopub.status.busy":"2024-09-23T23:08:24.700966Z","iopub.status.idle":"2024-09-23T23:08:29.971355Z","shell.execute_reply":"2024-09-23T23:08:29.954004Z","shell.execute_reply.started":"2024-09-23T23:08:24.701530Z"},"language":"python","trusted":true},"outputs":[],"source":"!pip install tensorflow==2.12.0 --quiet"},{"cell_type":"code","execution_count":6,"id":"d451de33-0d4e-4935-b1c2-9b058217af4a","metadata":{"execution":{"iopub.execute_input":"2024-09-23T23:08:33.204988Z","iopub.status.busy":"2024-09-23T23:08:33.204411Z","iopub.status.idle":"2024-09-23T23:08:33.211654Z","shell.execute_reply":"2024-09-23T23:08:33.211050Z","shell.execute_reply.started":"2024-09-23T23:08:33.204951Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"env: TF_ENABLE_ONEDNN_OPTS=0\nenv: TF_CPP_MIN_LOG_LEVEL=2\n"}],"source":"%env TF_ENABLE_ONEDNN_OPTS=0\n%env TF_CPP_MIN_LOG_LEVEL=2"},{"attachments":{},"cell_type":"markdown","id":"7419f1dd-ed5e-4ba1-822d-bef9962f0bf1","metadata":{"language":"python"},"source":"## encoder"},{"cell_type":"code","execution_count":8,"id":"7b470e76-ffc0-48d4-967e-0300eda9e10a","metadata":{"execution":{"iopub.execute_input":"2024-09-23T23:08:37.559306Z","iopub.status.busy":"2024-09-23T23:08:37.558806Z","iopub.status.idle":"2024-09-23T23:08:37.596136Z","shell.execute_reply":"2024-09-23T23:08:37.595665Z","shell.execute_reply.started":"2024-09-23T23:08:37.559264Z"},"language":"python","trusted":true},"outputs":[],"source":"\"\"\"Byte pair encoding utilities.\n\nCopied from: https://github.com/openai/gpt-2/blob/master/src/encoder.py.\n\"\"\"\nimport json\nimport os\nfrom functools import lru_cache\n\nimport regex as re\n\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a significant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8 + n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\ndef get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\nclass Encoder:\n    def __init__(self, encoder, bpe_merges, errors=\"replace\"):\n        self.encoder = encoder\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.errors = errors  # how to handle errors in decoding\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n        self.cache = {}\n\n        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n                    new_word.append(first + second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \" \".join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        for token in re.findall(self.pat, text):\n            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \"))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = \"\".join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n        return text\n\ndef get_encoder(model_name, models_dir):\n    with open(os.path.join(models_dir, model_name, \"encoder.json\"), \"r\") as f:\n        encoder = json.load(f)\n    with open(os.path.join(models_dir, model_name, \"vocab.bpe\"), \"r\", encoding=\"utf-8\") as f:\n        bpe_data = f.read()\n    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]\n    return Encoder(encoder=encoder, bpe_merges=bpe_merges)"},{"attachments":{},"cell_type":"markdown","id":"f2302261-dcf6-4489-a629-1feb222770bb","metadata":{"language":"python"},"source":"## utils"},{"cell_type":"code","execution_count":9,"id":"ae268319-b485-4dfd-b777-1c2b673b38d4","metadata":{"execution":{"iopub.execute_input":"2024-09-23T23:08:42.596753Z","iopub.status.busy":"2024-09-23T23:08:42.596444Z","iopub.status.idle":"2024-09-23T23:08:46.024001Z","shell.execute_reply":"2024-09-23T23:08:46.023218Z","shell.execute_reply.started":"2024-09-23T23:08:42.596726Z"},"language":"python","trusted":true},"outputs":[],"source":"import json\nimport os\nimport regex as re\n\nimport numpy as np\nimport requests\nimport tensorflow as tf\nfrom tqdm import tqdm\n\ndef download_gpt2_files(model_size, model_dir):\n    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n    for filename in [\n        \"checkpoint\",\n        \"encoder.json\",\n        \"hparams.json\",\n        \"model.ckpt.data-00000-of-00001\",\n        \"model.ckpt.index\",\n        \"model.ckpt.meta\",\n        \"vocab.bpe\",\n    ]:\n        url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n        r = requests.get(f\"{url}/{model_size}/{filename}\", stream=True)\n        r.raise_for_status()\n\n        with open(os.path.join(model_dir, filename), \"wb\") as f:\n            file_size = int(r.headers[\"content-length\"])\n            chunk_size = 1000\n            with tqdm(\n                ncols=100,\n                desc=\"Fetching \" + filename,\n                total=file_size,\n                unit_scale=True,\n                unit=\"b\",\n            ) as pbar:\n                # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n                for chunk in r.iter_content(chunk_size=chunk_size):\n                    f.write(chunk)\n                    pbar.update(chunk_size)\n\ndef load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams):\n    def set_in_nested_dict(d, keys, val):\n        if not keys:\n            return val\n        if keys[0] not in d:\n            d[keys[0]] = {}\n        d[keys[0]] = set_in_nested_dict(d[keys[0]], keys[1:], val)\n        return d\n\n    params = {\"blocks\": [{} for _ in range(hparams[\"n_layer\"])]}\n    for name, _ in tf.train.list_variables(tf_ckpt_path):\n        array = np.squeeze(tf.train.load_variable(tf_ckpt_path, name))\n        name = name[len(\"model/\") :]\n        if name.startswith(\"h\"):\n            m = re.match(r\"h([0-9]+)/(.*)\", name)\n            n = int(m[1])\n            sub_name = m[2]\n            set_in_nested_dict(params[\"blocks\"][n], sub_name.split(\"/\"), array)\n        else:\n            set_in_nested_dict(params, name.split(\"/\"), array)\n\n    return params\n\ndef load_encoder_hparams_and_params(model_size, models_dir):\n    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n\n    model_dir = os.path.join(models_dir, model_size)\n    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n    if not tf_ckpt_path:  # download files if necessary\n        os.makedirs(model_dir, exist_ok=True)\n        download_gpt2_files(model_size, model_dir)\n        tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n\n    encoder = get_encoder(model_size, models_dir)\n    hparams = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams)\n\n    return encoder, hparams, params"},{"attachments":{},"cell_type":"markdown","id":"8b7416ae-805d-4a36-9a2a-c21f64e57a63","metadata":{"language":"python"},"source":"## gpt2"},{"cell_type":"code","execution_count":10,"id":"63deb11b-27d7-4081-a1d9-c4f2f36a6e31","metadata":{"execution":{"iopub.execute_input":"2024-09-23T23:08:52.458677Z","iopub.status.busy":"2024-09-23T23:08:52.458019Z","iopub.status.idle":"2024-09-23T23:08:52.474156Z","shell.execute_reply":"2024-09-23T23:08:52.473460Z","shell.execute_reply.started":"2024-09-23T23:08:52.458636Z"},"language":"python","trusted":true},"outputs":[],"source":"import numpy as np\n\ndef gelu(x):\n    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\ndef layer_norm(x, g, b, eps: float = 1e-5):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    x = (x - mean) / np.sqrt(variance + eps)  # normalize x to have mean=0 and var=1 over last axis\n    return g * x + b  # scale and offset with gamma/beta params\n\ndef linear(x, w, b):  # [m, in], [in, out], [out] -> [m, out]\n    return x @ w + b\n\ndef ffn(x, c_fc, c_proj):  # [n_seq, n_embd] -> [n_seq, n_embd]\n    # project up\n    a = gelu(linear(x, **c_fc))  # [n_seq, n_embd] -> [n_seq, 4*n_embd]\n\n    # project back down\n    x = linear(a, **c_proj)  # [n_seq, 4*n_embd] -> [n_seq, n_embd]\n\n    return x\n\ndef attention(q, k, v, mask):  # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -> [n_q, d_v]\n    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n\ndef mha(x, c_attn, c_proj, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n    # qkv projection\n    x = linear(x, **c_attn)  # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n\n    # split into qkv\n    qkv = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -> [3, n_seq, n_embd]\n\n    # split into heads\n    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  # [3, n_seq, n_embd] -> [3, n_head, n_seq, n_embd/n_head]\n\n    # causal mask to hide future inputs from being attended to\n    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq]\n\n    # perform attention over each head\n    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]  # [3, n_head, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head]\n\n    # merge heads\n    x = np.hstack(out_heads)  # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n\n    # out projection\n    x = linear(x, **c_proj)  # [n_seq, n_embd] -> [n_seq, n_embd]\n\n    return x\n\ndef transformer_block(x, mlp, attn, ln_1, ln_2, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n    # multi-head causal self attention\n    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n\n    # position-wise feed forward network\n    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -> [n_seq, n_embd]\n\n    return x\n\ndef gpt2(inputs, wte, wpe, blocks, ln_f, n_head):  # [n_seq] -> [n_seq, n_vocab]\n    # token + positional embeddings\n    x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n\n    # forward pass through n_layer transformer blocks\n    for block in blocks:\n        x = transformer_block(x, **block, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n\n    # projection to vocab\n    x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]\n    return x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]\n\ndef generate(inputs, params, n_head, n_tokens_to_generate):\n    from tqdm import tqdm\n\n    for _ in tqdm(range(n_tokens_to_generate), \"generating\"):  # auto-regressive decode loop\n        logits = gpt2(inputs, **params, n_head=n_head)  # model forward pass\n        next_id = np.argmax(logits[-1])  # greedy sampling\n        inputs.append(int(next_id))  # append prediction to input\n\n    return inputs[len(inputs) - n_tokens_to_generate :]  # only return generated ids\n\ndef main(prompt: str, n_tokens_to_generate: int = 40, model_size: str = \"124M\", models_dir: str = \"models\"):\n\n    # load encoder, hparams, and params from the released open-ai gpt-2 files\n    encoder, hparams, params = load_encoder_hparams_and_params(model_size, models_dir)\n\n    # encode the input string using the BPE tokenizer\n    input_ids = encoder.encode(prompt)\n\n    # make sure we are not surpassing the max sequence length of our model\n    assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n\n    # generate output ids\n    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n\n    # decode the ids back into a string\n    output_text = encoder.decode(output_ids)\n\n    return output_text"},{"cell_type":"code","execution_count":11,"id":"3bceda20-42fd-41b0-8cf6-36802860c62e","metadata":{"execution":{"iopub.execute_input":"2024-09-23T23:08:56.687224Z","iopub.status.busy":"2024-09-23T23:08:56.686841Z","iopub.status.idle":"2024-09-23T23:09:06.217918Z","shell.execute_reply":"2024-09-23T23:09:06.216748Z","shell.execute_reply.started":"2024-09-23T23:08:56.687197Z"},"language":"python","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":"generating: 100%|██████████| 40/40 [00:08<00:00,  4.54it/s]"},{"name":"stdout","output_type":"stream","text":" the most powerful machines on the planet.\n\nThe computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain.\n\n"},{"name":"stderr","output_type":"stream","text":"\n"}],"source":"result = main(\"Alan Turing theorized that computers would one day become\")\nprint(result)"},{"cell_type":"code","execution_count":12,"id":"f51ad776-9d9e-4ded-ba7b-c333acdf212a","metadata":{"execution":{"iopub.execute_input":"2024-09-23T23:09:15.747117Z","iopub.status.busy":"2024-09-23T23:09:15.746531Z","iopub.status.idle":"2024-09-23T23:09:24.871230Z","shell.execute_reply":"2024-09-23T23:09:24.870221Z","shell.execute_reply.started":"2024-09-23T23:09:15.747083Z"},"language":"python","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":"generating: 100%|██████████| 40/40 [00:08<00:00,  4.67it/s]"},{"name":"stdout","output_type":"stream","text":" they are the only members of the family that live in the desert.\n\nThe camelid family is a group of animals that live in the desert. The camelid family is a group of animals\n"},{"name":"stderr","output_type":"stream","text":"\n"}],"source":"result = main(\"Llamas are members of the camelid family meaning\")\nprint(result)"},{"attachments":{},"cell_type":"markdown","id":"b25e1ac8-c6f2-481c-900d-c835c593fe4e","metadata":{"language":"python"},"source":"[picoGPT](https://github.com/jaymody/picoGPT)\n\n## MIT License\n\nCopyright (c) 2023 Jay Mody\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."}],"metadata":{"jupyterlab":{"notebooks":{"version_major":6,"version_minor":4}},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"singlestore_cell_default_language":"python","singlestore_connection":{"connectionID":"7f855434-7978-4870-a28c-e91df490846c","defaultDatabase":""},"singlestore_row_limit":300},"nbformat":4,"nbformat_minor":5}